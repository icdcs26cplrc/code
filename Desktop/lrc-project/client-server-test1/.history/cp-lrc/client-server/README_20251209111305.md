# client-server
# Cascade-Parity LRC Client/Server

This repository implements a Local Reconstruction Codes (LRC) prototype with a client–proxy–coordinator–datanode architecture. It provides encoding data into stripes, distribute blocks, and repairing failures using Jerasure-based LRC routines.

Use the steps below to build, configure, and run the system based on the code in this repo.

## Prerequisites

Install the required libraries before building:

- **Jerasure** (LRC math; builds `libJerasure`)
  ```bash
  cd third_party/jerasure
  mkdir -p build && cd build
  cmake .. && make
  ```
- **ASIO** (header-only, already in `third_party/asio`)
- **OpenSSL** (link with `-lssl -lcrypto`)
- A `g++` compiler with C++17 support

## Build

Compile all binaries from `src` (artifacts are written to `target/`):

```bash
cd src
make all
```

Key outputs:

- `target/client`: interactive CLI for set/get/repair
- `target/proxy`: proxy node
- `target/coordinator`: coordinator node
- `target/datanode` and `target/datanode_single`: datanode processes (the single variant is convenient for local demos)

## Configuration

All components read the shared `include/config.xml`. Update the network layout and LRC parameters to match your deployment:

| Parameter | Description | Example/Default |
| --- | --- | --- |
| `client.addr` | Client bind address | `192.168.1.243` |
| `proxy.addr` | Proxy address | `192.168.0.45` |
| `coordinator.addr` | Coordinator address | `192.168.1.244` |
| `datanode.addr` | List of 15 datanode addresses | See the sample list in file |
| `datanode.ports` | Ports for each datanode (aligned with addresses) | Sequence starting at `34567` |
| `client.port` | Client listen port | `12345` |
| `coordinator.port` | Coordinator listen port | `12378` |
| `proxy.port` / `proxy.port.data` | Proxy control/data ports | `34589` / `35645` |
| `proxy.port.base`, `datanode.port.base` | Port bases used for assignment | `23456`, `34567` |

Distribute the same `config.xml` to the `include/` directory on every host so all binaries load a consistent configuration.

## Optional test data

You can create sample files for uploads using the helper script:

```bash
cd src
python generate_test_data.py
# Follow the prompts for output directory, file count, and size range
```

## Running the system

Typical startup order:

1. Start datanodes on their respective hosts. For a single-process demo:
   ```bash
   # node_id ip_index port_index k r p block_size
   ./target/datanode_single 0 0 0 10 4 2 1048576
   ```
   Arguments map to entries in `config.xml`: `k` data blocks, `r` global parity blocks, `p` local group size, and `block_size` in bytes.
2. On the proxy host, run `./target/proxy` and answer the prompt about I/O repair optimizations.
3. On the coordinator host, run `./target/coordinator`; it loads stripe/node info from `config.xml`.
4. On the client host, run `./target/client` and issue commands:
   - `set <file_path>`: read a local file and write it to the LRC cluster.
   - `get <file_name>`: fetch a file from the cluster.
   - `repair <stripe_x_block_y>`: trigger repair for a specific block.

## Notes

- Stripes are generated automatically on `set` requests; data is encoded and distributed to datanodes by the proxy and coordinator.
- To observe repair behavior, you can manually stop a datanode and issue a `repair` command from the client; the coordinator and proxy will schedule reconstruction using the remaining blocks.